---
title: "March Madness ML"
author: "Richard Hardis, Arjun Goyal, Fred Sackfield, Anthony Temeliescu"
date: "3/19/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Abstract

# Report

## Data Collection

The main portion of our data is taken from the Kaggle dataset called College Basketball Dataset by Andrew Sundberg (https://www.kaggle.com/andrewsundberg/college-basketball-dataset#cbb.csv).  It contains data on 24 different variables measuring relevant team performance metrics such as adjusted tempo, tournament seed, and defensive rebound percentage. In total, the dataset contains 1757 rows and includes 5 seasons of data from 2015 to 2019. A driving motivation of the project is to work with novel or unconventional data points to explore alternate angles of predicting NCAA tournament performance. To this end, we treated the kaggle dataset as our "base" set, which we then looked to supplement with additional features.

For these additional features, we came up with ideas for quantitative data points that we believed could be useful in predicting post-season performance. One such feature that we added is regular season performance against the spread (ATS). We believe this data point is not widely used in other NCAA tournament modeling efforts, and it provides an interesting perspective on a team's regular season performance based on market expectations. Another feature that we added was coach's past performance, both in regular season wins and tournament wins. We believe that a coach's pedigree and past tournament performance could be a significant factor in tournament performance. 

We wrote Python web scraping programs to collect ATS data and coaching data from Covers.com and KenPom.com, respectively. These scripts imported the new data into a SQL database, where the data was then cleaned and integrated with our base set from Kaggle. The resulting "full" dataset was then exported from SQL into a csv file, which will be used for the rest of the analysis below. 

## Data Exploration
```{r}
# Clear any old data
remove(list = ls())

# Write the status check file
fileConn<-file("status.txt")
writeLines("The file did not run successfully.", fileConn)
close(fileConn)

# Load in data
full_data = read.csv("cbb_full.csv")
full_data$G = full_data$G - full_data$POSTSEASON
full_data$W = full_data$W - full_data$POSTSEASON
full_data["AdjSpread"] = full_data$ADJOE-full_data$ADJDE
full_data = na.omit(full_data)
```

```{r, message=F, warning=F, echo=F}
# Perform data exploration, outlier detection, and produce any plots required
source("data_exploration.R")    # Loads functions from data exploration file

percentage_stats_plots()
adjusted_wins_plots()
games_wins_plots()

#data_no_outliers = remove_outliers(full_data)
#exploration / plots
```
The offensive percentage statistics were analyzed separately from the defensive percentage statistics to view outliers and trends. We see one team that shot at a higher field goal percentage (EFG_O) of 60% compared to other teams. There are a few teams that had a higher turnover rate (TOR) closer to 25% compared to themedian. ONe team had a significantly lower offensive rebound percentage (ORB), under 20%, compared to the median near 30%. The median two-point shooting percentage (X2P_0) is near 50% while two teams shot closer to 60%. We see that on defense, there were a few teams that caused more turnovers (TORD) compared to the median. While the median free throw rate allowed (FTRD) was near 30%, 6 teams allowed much higher rates near 50-55%. Two teams were good defenders against the two-point shooting of their opponents (X2P_D), only allowing under 40% compared to the median closer to 50%. 

To qualify for the tournament, most teams have a high adjusted offensive efficiency (ADJOE) that is higher than their adjusted defensive efficiency (ADJDE), indicating they should be able to score on division I teams effectively while preventing their opponents from scoring on them. The boxplot for BARTHAG indicates that the median chance of defeating the average division I opponent is roughly 90%. However, there are many outliers that are below 50% given that the tournament is open to 64 teams, many of which are from small basketball programs and less-competitive conferences. 

Most teams that qualify for the tournament play and win roughly the same amount of games during the season. Some teams achieve a higher ATS score than the median since the tournament frequently sees lower-seeded teams advance against the odds. We see that the median number a coach's previous tournament wins (PREV_TW) is near 0, however there are a few teams who possess coaches with 20-60 games of winning experience in the tournament. A coach's previous regular season wins sits at a median of 200 with no outliers indicated.

```{r, message=F, warning=F, echo=F}}
corr_plot()
```
From the correlation plot of the quantitative variables, we see that a team's adjusted offensive effiency (ADJOE) has a positive correlation with some of the team's offensive percentage stats such as EFG_O, X2P_O, X3P_O. There is a strong positive correlation with the coach's previous tournament and regular season wins, as well as the BARTHAG rating. Teams with a high ADJOE have a strongly correlated WAB as they are more likely to make the tournament. Expectedly, the adjusted defensive efficiency (ADJDE) is negatively correlated with these explanatory variables given that a lower ADJDE is desired. EFG_O is strongly positively correlated with the team's two-point and three-point shooting percentage (X2P_O and X3P_O), and conversely the team's EFG_D is strongly positively correlated with the team's ablility to defend the two-point and three-point shooting of their opponent (X2P_D, X3P_D). Many of the explanatory variables are only weakly correlated with each other. We do see a moderate positive correlation between the WAB and the coach's previous tournament and regular season wins, perhaps indicating the impact of a coach's previous experience on a team's performance.

```{r, message=F, warning=F, echo=F}
scaled_data = normalize_numerics(full_data)
scaled_data = cbind(cbind(with(scaled_data, model.matrix(~ CONF + 0))), scaled_data[,-which(names(scaled_data) %in% c("CONF"))])
cleaned_data.team = scaled_data[complete.cases(scaled_data),]
cleaned_data = cleaned_data.team[,-which(names(scaled_data) %in% c("TEAM"))]
```
## Methods

  In order to predict the number of tournament games won by a given team, several models were fit and assessed.  This prediction is a regression-type problem as the target variable is continuous over the range [0,6].  The five model types chosen for this task were lasso regression, ridge regression, multilinear regression, Multivariate Adaptive Regression Splines (MARS), boosting, and random forest.  For models with varying parameter values, such as lambda in lasso regression, an initial cross validation was run with built in cross validation functionality to find the best parameter values.  Once the best parameter models were gathered, all of the models were compared against each other using ten fold K-folds cross validation.  Two separate metrics were used to score the models.  The first is the mean squared error (MSE) of the model's prediction against the actual number of NCAA tournament games won.  The second is a custom metric called "accuracy" that measures the percentage of games correctly predicted within plus or minus a half game of the true value.  This metric was developed because it may have more applicability to game winner prediction if this model is used to fill out a March Madness Bracket than a pure MSE score.  Each model was scored and the average score for the ten cross validation folds is displayed in the table below.

```{r}
# Load in algorithms from algo file
source("cross-validation.R")

k_mets = kfolds_cv(15, cleaned_data, run_boost=F)

colnames(k_mets) = c("Lasso","Ridge","MLR","MARS","Boosting", "Random Forest")
rownames(k_mets) = c("MSE", "+/- 0.5 Game Prediction Accuracy")
k_mets=round(k_mets,3)
k_mets
```

## Results

  It can be seen from the above table that the best model was Ridge Regression.  Ridge regression had the highest accuracy and lowest MSE score.  The accuracy of this model is not very high, only guessing the number of games won (when rounded to the nearest integer) 59% of the time.  While this is better than a random guess, it is not much higher.  The Ridge Regression model was then retrained on the full cleaned dataset and predictions for all games in the dataset were made.  The coefficients for the model are found in the output below.


```{r}
library(glmnet)
library(MLmetrics)
train.x.mat = as.matrix(cleaned_data[,-which(names(cleaned_data) %in% c("POSTSEASON"))])
train.y.mat = cleaned_data[,which(names(cleaned_data) %in% c("POSTSEASON"))]
test.y.mat = cleaned_data[,which(names(cleaned_data) %in% c("POSTSEASON"))]
lambdas = 10^seq(3, -2, by = -.1)
ridge.model_cv = cv.glmnet(train.x.mat, train.y.mat, alpha=1, lambda = lambdas)
best_lambda = ridge.model_cv$lambda.min
ridge.model = glmnet(train.x.mat, train.y.mat, alpha=1, lambda = best_lambda)
pred.best_ridge = predict(ridge.model, s=best_lambda, newx=train.x.mat)
MSE(pred.best_ridge, test.y.mat)
correct_games(pred.best_ridge, test.y.mat)
ridge.model$beta
```

```{r}
library(glmnet)
library(caret)
library(earth)
library(MLmetrics)
library(dplyr)
library(xgboost)
library(randomForest)

# Correct games scoring function
correct_games = function(preds, ytrue){
  # Returns the decimal value of game predictions strictly within 0.5 games of the true number of wins
  return(sum(abs(preds-ytrue)<0.5)/length(ytrue))
}

test.data = cleaned_data
train.data = cleaned_data

train.x.mat = as.matrix(train.data[,-which(names(full_df) %in% c("POSTSEASON"))])
train.y.mat = train.data[,which(names(full_df) %in% c("POSTSEASON"))]
test.x.mat = as.matrix(test.data[,-which(names(full_df) %in% c("POSTSEASON"))])
test.y.mat = test.data[,which(names(full_df) %in% c("POSTSEASON"))]


# Get predictions for each model
lasso.model = glmnet(train.x.mat, train.y.mat, alpha=1, lambda = best_lasso_lambda)
pred.best_lasso = predict(lasso.model, s=best_lasso_lambda, newx=test.x.mat)

ridge.model = glmnet(train.x.mat, train.y.mat, alpha=0, lambda = best_ridge_lambda)
pred.best_ridge = predict(ridge.model, s=best_ridge_lambda, newx=test.x.mat)
  
linreg.model = lm(POSTSEASON~., data = train.data)
pred.linreg = predict(linreg.model, newdata = as.data.frame(test.x.mat))

mars.model = train(POSTSEASON~., data=train.data, method="earth")
pred.mars = mars.model %>% predict(as.data.frame(test.x.mat))

boost.model = train(POSTSEASON~., data=train.data, method="xgbTree")
pred.boost = boost.model %>% predict(as.data.frame(test.x.mat))
mse.boost = MSE(pred.boost, test.y.mat)
acc.boost = correct_games(pred.boost, test.y.mat)

rf = randomForest(POSTSEASON~.,data=train.data)
pred.rf = predict(rf, newdata=test.data[,-which(names(full_df) %in% c("POSTSEASON"))])
```


## Conclusions
  Even with advanced data mining techniques, predicting March Madness team wins remains difficult to do with good accuracy.  Despite this, these techniques are still valuable in two applications.  The first is for team coaches. The March Madness performance ridge regression model coefficients can be used to determine which factors are useful in improving team tournament wins.  All other factors held constant, several factors provide outside improvements or hinderances on win numbers.  Teams from stronger conferences such as the ACC or Big 12 have higher win predictions.  Conversely, conferences such as Big Sky or CAA are negatively impacted by their conference.  This is likely due to top conferences putting teams through a more difficult regular season schedule, having better talent overall, and having good coaches.  For a coach of a team in a less competitive conference, this signifies that the team may get some benefit from scheduling early-season out of conference games against opponents from strong conferences to help diminish the effect of playing conference games against weaker opponents.  

```{r, echo=F, warning=F, message=F}
output = cbind(cleaned_data.team[, which(names(cleaned_data.team) %in% c("YEAR", "TEAM", "POSTSEASON"))], pred.best_ridge)
write.csv(output[output$YEAR == "2019",], "lasso_predictions.csv", row.names = F)
```

```{r, echo=F, warning=F, message=F}
# Status check.  Did the code reach this point?
# Write the status check file
fileConn<-file("status.txt")
writeLines("The file ran successfully.", fileConn)
close(fileConn)
```

